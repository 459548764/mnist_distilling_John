{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Define a Student Network and Distilling from Teacher Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap\n",
    "\n",
    "# set gpu device\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Define a Student Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "train data : (55000, 784)\n",
      "test  data : (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#   prepare the data\n",
    "data=\"D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/\"\n",
    "mnist = input_data.read_data_sets(data, one_hot=True)\n",
    "print(\"train data :\",mnist.train.images.shape)\n",
    "print(\"test  data :\",mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_ = tf.placeholder(tf.float32,[None, 784], name='Data')\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name='s_label')\n",
    "y_soft_target = tf.placeholder(tf.float32, [None, 10], name='sf_target')\n",
    "T = tf.placeholder(tf.float32, name='tempalate')\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 800]),name='s_W1')\n",
    "b1 = tf.Variable(tf.zeros([800]),name='s_b1')\n",
    "h_1 = tf.matmul(x_, W1,name='h_1')\n",
    "h_1 = tf.nn.relu(tf.add(h_1, b1,name='h_1_b'))\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([800,300]),name='s_W2')\n",
    "b2 = tf.Variable(tf.zeros([300]),name='s_b2')\n",
    "h_2 = tf.matmul(h_1, W2, name='h_2')\n",
    "h_2 = tf.nn.relu(tf.add(h_2, b2 , name='h_2_b'))\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([300,10]), name='s_W3')\n",
    "b3 = tf.Variable(tf.zeros([10]), name='s_b3')\n",
    "logits = tf.matmul(h_2, W3, name='s_logits')\n",
    "logits = tf.add(logits, b3, name='s_logits_b')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define params\n",
    "alpha = 0.1\n",
    "logits_pre = logits - tf.reduce_max(logits)\n",
    "# define hard Loss\n",
    "#hard_loss = tf.nn.softmax(logits,name='hl_softmax')\n",
    "#hard_loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(hard_loss),reduction_indices=[1],name='hd_loss')) \n",
    "hard_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_pre,labels=y_, name='hd_loss'))\n",
    "\n",
    "# define soft Loss\n",
    "#soft_loss = tf.nn.softmax(logits/T, name='sf_softmax')\n",
    "#soft_loss = tf.reduce_mean(-tf.reduce_sum(y_soft_target * tf.log(soft_loss), reduction_indices=[1], name='sf_loss'))\n",
    "soft_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_pre, labels=y_soft_target, name='soft_loss'))\n",
    "\n",
    "# regularization\n",
    "reg_loss = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2)) #+  tf.reduce_sum(tf.abs(W3))\n",
    "\n",
    "# define Loss\n",
    "Loss = hard_loss * alpha + soft_loss *(1-alpha) * tf.pow(T,2)# + 0.0001*reg_loss\n",
    "\n",
    "# Define global step\n",
    "model_global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step_s')\n",
    "\n",
    "# Define Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-2).minimize(Loss,global_step=model_global_step)\n",
    "\n",
    "correct_prediction =tf.equal(tf.argmax(logits,1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name='accurarcy_s')\n",
    "\n",
    "# saver model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. load teacher  network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Then, we import the graph_def into a new Graph and returns it \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # The name var will prefix every op/nodes in your graph\n",
    "        # Since we load everything in a new graph, this is not needed\n",
    "        tf.import_graph_def(graph_def, name=\"prefix\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print teacher node name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = load_graph(\"./frozen_model.pb\")\n",
    "#for op in graph.get_operations():\n",
    "       # pass\n",
    "        #print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher network accuracy = 0.9936\n",
      "step 0, training accuracy 0.31 , loss = 1119.16 , hard_loss = 1119.16, soft_loss = 1119.16\n",
      "test accuracy 0.1897\n",
      "step 200, training accuracy 0.95 , loss = 22.2803 , hard_loss = 22.28, soft_loss = 22.2804\n",
      "step 400, training accuracy 0.96 , loss = 6.9683 , hard_loss = 6.96825, soft_loss = 6.96831\n",
      "step 600, training accuracy 0.97 , loss = 2.4665 , hard_loss = 2.46018, soft_loss = 2.4672\n",
      "step 800, training accuracy 0.96 , loss = 5.15508 , hard_loss = 5.15354, soft_loss = 5.15525\n",
      "step 1000, training accuracy 0.97 , loss = 5.36967 , hard_loss = 5.38126, soft_loss = 5.36838\n",
      "test accuracy 0.9459\n",
      "step 1200, training accuracy 0.98 , loss = 0.232698 , hard_loss = 0.232316, soft_loss = 0.232741\n",
      "step 1400, training accuracy 0.99 , loss = 0.539338 , hard_loss = 0.532792, soft_loss = 0.540065\n",
      "step 1600, training accuracy 0.97 , loss = 2.21025 , hard_loss = 2.20961, soft_loss = 2.21032\n",
      "step 1800, training accuracy 0.98 , loss = 1.74744 , hard_loss = 1.76253, soft_loss = 1.74576\n",
      "step 2000, training accuracy 0.96 , loss = 5.75712 , hard_loss = 5.75217, soft_loss = 5.75767\n",
      "test accuracy 0.9553\n",
      "step 2200, training accuracy 1 , loss = 0.00132542 , hard_loss = 3.23618e-05, soft_loss = 0.00146909\n",
      "step 2400, training accuracy 0.98 , loss = 0.236037 , hard_loss = 0.232844, soft_loss = 0.236391\n",
      "step 2600, training accuracy 0.96 , loss = 2.39063 , hard_loss = 2.3891, soft_loss = 2.3908\n",
      "step 2800, training accuracy 0.99 , loss = 1.18137 , hard_loss = 1.18086, soft_loss = 1.18143\n",
      "step 3000, training accuracy 0.99 , loss = 0.275601 , hard_loss = 0.250744, soft_loss = 0.278362\n",
      "test accuracy 0.9532\n",
      "step 3200, training accuracy 1 , loss = 0.000509065 , hard_loss = 0.000162178, soft_loss = 0.000547608\n",
      "step 3400, training accuracy 1 , loss = 0.0089846 , hard_loss = 0.00599487, soft_loss = 0.00931679\n",
      "step 3600, training accuracy 0.96 , loss = 0.51332 , hard_loss = 0.513015, soft_loss = 0.513353\n",
      "step 3800, training accuracy 0.97 , loss = 0.942994 , hard_loss = 0.942929, soft_loss = 0.943001\n",
      "step 4000, training accuracy 1 , loss = 0.000733732 , hard_loss = 0.000499179, soft_loss = 0.000759793\n",
      "test accuracy 0.9483\n",
      "step 4200, training accuracy 0.99 , loss = 0.0230645 , hard_loss = 0.0209053, soft_loss = 0.0233044\n",
      "step 4400, training accuracy 0.99 , loss = 0.0263003 , hard_loss = 0.0255344, soft_loss = 0.0263854\n",
      "step 4600, training accuracy 0.96 , loss = 0.139107 , hard_loss = 0.139044, soft_loss = 0.139114\n",
      "step 4800, training accuracy 0.99 , loss = 0.0883629 , hard_loss = 0.0877696, soft_loss = 0.0884288\n",
      "step 5000, training accuracy 0.99 , loss = 0.0488783 , hard_loss = 0.0487868, soft_loss = 0.0488884\n",
      "test accuracy 0.9403\n",
      "step 5200, training accuracy 1 , loss = 0.00527159 , hard_loss = 0.00520902, soft_loss = 0.00527854\n",
      "step 5400, training accuracy 0.96 , loss = 0.118097 , hard_loss = 0.117966, soft_loss = 0.118112\n",
      "step 5600, training accuracy 0.95 , loss = 0.281008 , hard_loss = 0.279431, soft_loss = 0.281184\n",
      "step 5800, training accuracy 0.99 , loss = 0.072558 , hard_loss = 0.0725486, soft_loss = 0.072559\n",
      "step 6000, training accuracy 0.97 , loss = 0.0994111 , hard_loss = 0.0990974, soft_loss = 0.099446\n",
      "test accuracy 0.9408\n",
      "step 6200, training accuracy 0.93 , loss = 0.313325 , hard_loss = 0.313267, soft_loss = 0.313331\n",
      "step 6400, training accuracy 0.99 , loss = 0.014985 , hard_loss = 0.0148655, soft_loss = 0.0149983\n",
      "step 6600, training accuracy 0.96 , loss = 0.17829 , hard_loss = 0.178247, soft_loss = 0.178295\n",
      "step 6800, training accuracy 0.99 , loss = 0.0303851 , hard_loss = 0.0301329, soft_loss = 0.0304132\n",
      "step 7000, training accuracy 0.96 , loss = 0.39359 , hard_loss = 0.393045, soft_loss = 0.393651\n",
      "test accuracy 0.9311\n",
      "step 7200, training accuracy 0.93 , loss = 0.183384 , hard_loss = 0.180687, soft_loss = 0.183684\n",
      "step 7400, training accuracy 0.98 , loss = 0.0966997 , hard_loss = 0.0960969, soft_loss = 0.0967667\n",
      "step 7600, training accuracy 0.95 , loss = 0.18497 , hard_loss = 0.184897, soft_loss = 0.184978\n",
      "step 7800, training accuracy 0.98 , loss = 0.122402 , hard_loss = 0.121304, soft_loss = 0.122524\n",
      "step 8000, training accuracy 0.94 , loss = 0.149832 , hard_loss = 0.149756, soft_loss = 0.149841\n",
      "test accuracy 0.9506\n",
      "step 8200, training accuracy 0.98 , loss = 0.338992 , hard_loss = 0.338065, soft_loss = 0.339095\n",
      "step 8400, training accuracy 0.97 , loss = 0.223136 , hard_loss = 0.222832, soft_loss = 0.223169\n",
      "step 8600, training accuracy 0.99 , loss = 0.0333474 , hard_loss = 0.0333213, soft_loss = 0.0333503\n",
      "step 8800, training accuracy 0.92 , loss = 0.20429 , hard_loss = 0.204252, soft_loss = 0.204294\n",
      "step 9000, training accuracy 0.95 , loss = 0.152863 , hard_loss = 0.152746, soft_loss = 0.152876\n",
      "test accuracy 0.9391\n",
      "step 9200, training accuracy 0.98 , loss = 0.113837 , hard_loss = 0.113821, soft_loss = 0.113839\n",
      "step 9400, training accuracy 0.94 , loss = 0.171947 , hard_loss = 0.171261, soft_loss = 0.172023\n",
      "step 9600, training accuracy 1 , loss = 0.00826823 , hard_loss = 0.00795561, soft_loss = 0.00830296\n",
      "step 9800, training accuracy 0.96 , loss = 0.100342 , hard_loss = 0.100294, soft_loss = 0.100347\n",
      "step 10000, training accuracy 0.97 , loss = 0.085046 , hard_loss = 0.0849612, soft_loss = 0.0850554\n",
      "test accuracy 0.9615\n",
      "step 10200, training accuracy 0.96 , loss = 0.105309 , hard_loss = 0.105236, soft_loss = 0.105317\n",
      "step 10400, training accuracy 0.93 , loss = 0.173408 , hard_loss = 0.173301, soft_loss = 0.17342\n",
      "step 10600, training accuracy 0.95 , loss = 0.151793 , hard_loss = 0.151508, soft_loss = 0.151825\n",
      "step 10800, training accuracy 0.94 , loss = 0.151015 , hard_loss = 0.150955, soft_loss = 0.151022\n",
      "Finally - test accuracy 0.943\n"
     ]
    }
   ],
   "source": [
    "# teacher input tensor\n",
    "x_input = graph.get_tensor_by_name(\"prefix/x_placeholder:0\")\n",
    "keep_prob = graph.get_tensor_by_name(\"prefix/keep_prob:0\")\n",
    "# teacher predict tensor\n",
    "y_out = graph.get_tensor_by_name(\"prefix/y_conv:0\")\n",
    "\n",
    "# set T params\n",
    "params_t = 1\n",
    "\n",
    "sess_teacher = tf.Session(graph=graph)\n",
    "pred = sess_teacher.run(y_out, feed_dict={x_input: mnist.test.images, keep_prob:1.0})\n",
    "pred_np = np.argmax(pred,1)\n",
    "target = np.argmax(mnist.test.labels, 1)\n",
    "correct_prediction = np.sum(pred_np == target)\n",
    "print(\"teacher network accuracy =\" , correct_prediction /target.shape[0])\n",
    "\n",
    "tf.device(\"/gpu:0\")\n",
    "with tf.Session() as sess_student:\n",
    "    sess_student.run(tf.global_variables_initializer())\n",
    "    for i in range(11000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        \n",
    "        # teacher soft_target    \n",
    "        soft_target = sess_teacher.run(y_out, feed_dict={x_input: batch[0], keep_prob:1.0})\n",
    "        soft_target = tf.nn.softmax(soft_target/params_t)\n",
    "        \n",
    "        # student train processing\n",
    "        train_step.run(feed_dict={x_ :batch[0], y_: batch[1], T : params_t, y_soft_target:soft_target.eval() })\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            hd_loss, sf_loss, loss_num, train_accuracy = sess_student.run([hard_loss, soft_loss ,Loss, accuracy], \n",
    "                                                        feed_dict={x_:batch[0],  y_:batch[1],\n",
    "                                                                   T:1.0, y_soft_target:soft_target.eval()  }) \n",
    "            print('step %d, training accuracy %g , loss = %g , hard_loss = %g, soft_loss = %g' % \n",
    "                       (i, train_accuracy, loss_num, hd_loss, sf_loss ))\n",
    "        if i % 1000 == 0:\n",
    "            print('test accuracy %g' % sess_student.run(accuracy,feed_dict={\n",
    "                                    x_: mnist.test.images, y_: mnist.test.labels, T: 1.0}))\n",
    "            \n",
    "    print('Finally - test accuracy %g' % sess_student.run(accuracy,feed_dict={\n",
    "                                    x_: mnist.test.images, y_: mnist.test.labels, T: 1.0}))\n",
    "           \n",
    "    \n",
    "sess_teacher.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
