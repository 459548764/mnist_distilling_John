{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Define a Student Network and Distilling from Teacher Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap\n",
    "\n",
    "# set gpu device\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Define a Student Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "train data : (55000, 784)\n",
      "test  data : (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#   prepare the data\n",
    "data=\"D:/Deep_Learning/TensorFlow/CS 20SI_youtube_video/class_notes/MNIST_data/\"\n",
    "mnist = input_data.read_data_sets(data, one_hot=True)\n",
    "print(\"train data :\",mnist.train.images.shape)\n",
    "print(\"test  data :\",mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_ = tf.placeholder(tf.float32,[None, 784], name='Data')\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name='s_label')\n",
    "y_soft_target = tf.placeholder(tf.float32, [None, 10], name='sf_target')\n",
    "T = tf.placeholder(tf.float32, name='tempalate')\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 800]),name='s_W1')\n",
    "b1 = tf.Variable(tf.zeros([800]),name='s_b1')\n",
    "h_1 = tf.matmul(x_, W1,name='h_1')\n",
    "h_1 = tf.nn.relu(tf.add(h_1, b1,name='h_1_b'))\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([800,300]),name='s_W2')\n",
    "b2 = tf.Variable(tf.zeros([300]),name='s_b2')\n",
    "h_2 = tf.matmul(h_1, W2, name='h_2')\n",
    "h_2 = tf.nn.relu(tf.add(h_2, b2 , name='h_2_b'))\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([300,10]), name='s_W3')\n",
    "b3 = tf.Variable(tf.zeros([10]), name='s_b3')\n",
    "logits = tf.matmul(h_2, W3, name='s_logits')\n",
    "logits = tf.add(logits, b3, name='s_logits_b')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define params\n",
    "alpha = 0.2\n",
    "logits_pre = logits - tf.reduce_max(logits)\n",
    "# define hard Loss\n",
    "#hard_loss = tf.nn.softmax(logits,name='hl_softmax')\n",
    "#hard_loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(hard_loss),reduction_indices=[1],name='hd_loss')) \n",
    "hard_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_pre,labels=y_, name='hd_loss'))\n",
    "\n",
    "# define soft Loss\n",
    "#soft_loss = tf.nn.softmax(logits/T, name='sf_softmax')\n",
    "#soft_loss = tf.reduce_mean(-tf.reduce_sum(y_soft_target * tf.log(soft_loss), reduction_indices=[1], name='sf_loss'))\n",
    "soft_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_pre, labels=y_soft_target, name='soft_loss'))\n",
    "\n",
    "# regularization\n",
    "reg_loss = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2)) #+  tf.reduce_sum(tf.abs(W3))\n",
    "\n",
    "# define Loss\n",
    "Loss = hard_loss * alpha + soft_loss *(1-alpha) * tf.pow(T,2)# + 0.0001*reg_loss\n",
    "#Loss = soft_loss * tf.pow(T,2)\n",
    "\n",
    "# Define global step\n",
    "model_global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step_s')\n",
    "\n",
    "# Define Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-2).minimize(Loss,global_step=model_global_step)\n",
    "\n",
    "correct_prediction =tf.equal(tf.argmax(logits,1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name='accurarcy_s')\n",
    "\n",
    "# saver model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. load teacher  network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Then, we import the graph_def into a new Graph and returns it \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        # The name var will prefix every op/nodes in your graph\n",
    "        # Since we load everything in a new graph, this is not needed\n",
    "        tf.import_graph_def(graph_def, name=\"prefix\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print teacher node name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = load_graph(\"./frozen_model.pb\")\n",
    "#for op in graph.get_operations():\n",
    "       # pass\n",
    "        #print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher network accuracy = 0.9936\n",
      "step 0, training accuracy 0.19 , loss = 1334.47 , hard_loss = 1334.46, soft_loss = 1334.47\n",
      "test accuracy 0.1476\n",
      "step 200, training accuracy 0.93 , loss = 18.2043 , hard_loss = 18.2189, soft_loss = 18.2006\n",
      "step 400, training accuracy 0.91 , loss = 15.8134 , hard_loss = 15.7865, soft_loss = 15.8201\n",
      "step 600, training accuracy 0.93 , loss = 6.80076 , hard_loss = 6.79877, soft_loss = 6.80126\n",
      "step 800, training accuracy 0.96 , loss = 2.09588 , hard_loss = 2.09475, soft_loss = 2.09616\n",
      "step 1000, training accuracy 0.93 , loss = 10.3224 , hard_loss = 10.324, soft_loss = 10.322\n",
      "test accuracy 0.9351\n",
      "step 1200, training accuracy 0.98 , loss = 4.34509 , hard_loss = 4.33701, soft_loss = 4.34711\n",
      "step 1400, training accuracy 0.96 , loss = 8.27815 , hard_loss = 8.28012, soft_loss = 8.27766\n",
      "step 1600, training accuracy 0.97 , loss = 3.5061 , hard_loss = 3.50482, soft_loss = 3.50642\n",
      "step 1800, training accuracy 0.99 , loss = 0.242274 , hard_loss = 0.241543, soft_loss = 0.242456\n",
      "step 2000, training accuracy 0.97 , loss = 1.22589 , hard_loss = 1.22358, soft_loss = 1.22647\n",
      "test accuracy 0.9507\n",
      "step 2200, training accuracy 1 , loss = 0.000999015 , hard_loss = 0.000253989, soft_loss = 0.00118527\n",
      "step 2400, training accuracy 0.99 , loss = 0.152385 , hard_loss = 0.150759, soft_loss = 0.152791\n",
      "step 2600, training accuracy 0.98 , loss = 0.508078 , hard_loss = 0.489156, soft_loss = 0.512809\n",
      "step 2800, training accuracy 1 , loss = 0.000705033 , hard_loss = 1.59739e-07, soft_loss = 0.000881251\n",
      "step 3000, training accuracy 0.99 , loss = 0.064523 , hard_loss = 0.0632772, soft_loss = 0.0648344\n",
      "test accuracy 0.9524\n",
      "step 3200, training accuracy 0.98 , loss = 0.0995158 , hard_loss = 0.0993386, soft_loss = 0.0995601\n",
      "step 3400, training accuracy 0.99 , loss = 0.289069 , hard_loss = 0.288994, soft_loss = 0.289088\n",
      "step 3600, training accuracy 0.97 , loss = 2.64658 , hard_loss = 2.62847, soft_loss = 2.65111\n",
      "step 3800, training accuracy 0.99 , loss = 0.176187 , hard_loss = 0.17469, soft_loss = 0.176561\n",
      "step 4000, training accuracy 0.99 , loss = 0.221401 , hard_loss = 0.220638, soft_loss = 0.221591\n",
      "test accuracy 0.9549\n",
      "step 4200, training accuracy 0.99 , loss = 0.189858 , hard_loss = 0.189269, soft_loss = 0.190006\n",
      "step 4400, training accuracy 0.98 , loss = 0.182744 , hard_loss = 0.180501, soft_loss = 0.183304\n",
      "step 4600, training accuracy 0.99 , loss = 0.559413 , hard_loss = 0.558913, soft_loss = 0.559539\n",
      "step 4800, training accuracy 0.99 , loss = 0.0343562 , hard_loss = 0.0342671, soft_loss = 0.0343785\n",
      "step 5000, training accuracy 0.96 , loss = 0.104731 , hard_loss = 0.104182, soft_loss = 0.104868\n",
      "test accuracy 0.953\n",
      "step 5200, training accuracy 1 , loss = 0.00115318 , hard_loss = 0.000643386, soft_loss = 0.00128063\n",
      "step 5400, training accuracy 1 , loss = 0.00265245 , hard_loss = 0.00254138, soft_loss = 0.00268022\n",
      "step 5600, training accuracy 0.99 , loss = 0.170603 , hard_loss = 0.173588, soft_loss = 0.169857\n",
      "step 5800, training accuracy 0.99 , loss = 0.342874 , hard_loss = 0.342659, soft_loss = 0.342928\n",
      "step 6000, training accuracy 0.99 , loss = 0.141775 , hard_loss = 0.141224, soft_loss = 0.141912\n",
      "test accuracy 0.9466\n",
      "step 6200, training accuracy 0.96 , loss = 0.200785 , hard_loss = 0.200623, soft_loss = 0.200826\n",
      "step 6400, training accuracy 0.98 , loss = 0.295123 , hard_loss = 0.294788, soft_loss = 0.295207\n",
      "step 6600, training accuracy 0.98 , loss = 0.132781 , hard_loss = 0.126641, soft_loss = 0.134316\n",
      "step 6800, training accuracy 0.97 , loss = 0.0688477 , hard_loss = 0.0686877, soft_loss = 0.0688877\n",
      "step 7000, training accuracy 0.98 , loss = 0.251924 , hard_loss = 0.266056, soft_loss = 0.24839\n",
      "test accuracy 0.9553\n",
      "step 7200, training accuracy 0.97 , loss = 0.760146 , hard_loss = 0.756339, soft_loss = 0.761097\n",
      "step 7400, training accuracy 0.99 , loss = 0.0399395 , hard_loss = 0.0396934, soft_loss = 0.040001\n",
      "step 7600, training accuracy 0.97 , loss = 0.0750137 , hard_loss = 0.0749481, soft_loss = 0.0750301\n",
      "step 7800, training accuracy 0.99 , loss = 0.11417 , hard_loss = 0.114106, soft_loss = 0.114187\n",
      "step 8000, training accuracy 0.98 , loss = 0.0941386 , hard_loss = 0.0938382, soft_loss = 0.0942137\n",
      "test accuracy 0.95\n",
      "step 8200, training accuracy 0.96 , loss = 0.139511 , hard_loss = 0.139379, soft_loss = 0.139544\n",
      "step 8400, training accuracy 0.98 , loss = 0.0478657 , hard_loss = 0.047591, soft_loss = 0.0479343\n",
      "step 8600, training accuracy 0.98 , loss = 0.14261 , hard_loss = 0.14259, soft_loss = 0.142615\n",
      "step 8800, training accuracy 0.98 , loss = 0.084973 , hard_loss = 0.0849944, soft_loss = 0.0849677\n",
      "step 9000, training accuracy 0.98 , loss = 0.078924 , hard_loss = 0.0786962, soft_loss = 0.078981\n",
      "test accuracy 0.9488\n",
      "step 9200, training accuracy 0.98 , loss = 0.0922516 , hard_loss = 0.0922331, soft_loss = 0.0922562\n",
      "step 9400, training accuracy 1 , loss = 0.022737 , hard_loss = 0.0227092, soft_loss = 0.022744\n",
      "step 9600, training accuracy 0.93 , loss = 0.194284 , hard_loss = 0.194281, soft_loss = 0.194284\n",
      "step 9800, training accuracy 0.97 , loss = 0.0877573 , hard_loss = 0.0877424, soft_loss = 0.087761\n",
      "step 10000, training accuracy 1 , loss = 0.0312103 , hard_loss = 0.0310969, soft_loss = 0.0312387\n",
      "test accuracy 0.9491\n",
      "step 10200, training accuracy 0.94 , loss = 0.174705 , hard_loss = 0.174565, soft_loss = 0.174741\n",
      "step 10400, training accuracy 0.99 , loss = 0.0473937 , hard_loss = 0.0462777, soft_loss = 0.0476727\n",
      "step 10600, training accuracy 0.99 , loss = 0.0362402 , hard_loss = 0.0360654, soft_loss = 0.0362839\n",
      "step 10800, training accuracy 0.98 , loss = 0.0634378 , hard_loss = 0.0583073, soft_loss = 0.0647205\n",
      "Finally - test accuracy 0.9549\n"
     ]
    }
   ],
   "source": [
    "# teacher input tensor\n",
    "x_input = graph.get_tensor_by_name(\"prefix/x_placeholder:0\")\n",
    "keep_prob = graph.get_tensor_by_name(\"prefix/keep_prob:0\")\n",
    "# teacher predict tensor\n",
    "y_out = graph.get_tensor_by_name(\"prefix/y_conv:0\")\n",
    "\n",
    "# set T params\n",
    "params_t = 1\n",
    "\n",
    "sess_teacher = tf.Session(graph=graph)\n",
    "pred = sess_teacher.run(y_out, feed_dict={x_input: mnist.test.images, keep_prob:1.0})\n",
    "pred_np = np.argmax(pred,1)\n",
    "target = np.argmax(mnist.test.labels, 1)\n",
    "correct_prediction = np.sum(pred_np == target)\n",
    "print(\"teacher network accuracy =\" , correct_prediction /target.shape[0])\n",
    "\n",
    "tf.device(\"/gpu:0\")\n",
    "with tf.Session() as sess_student:\n",
    "    sess_student.run(tf.global_variables_initializer())\n",
    "    for i in range(11000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        \n",
    "        # teacher soft_target\n",
    "    \n",
    "        soft_target = sess_teacher.run(y_out, feed_dict={x_input: batch[0], keep_prob:1.0})\n",
    "        soft_target = tf.nn.softmax(soft_target/params_t)\n",
    "        #print(\"shape \",soft_target.shape)\n",
    "        #print(\"value =\", soft_target.eval())\n",
    "        \n",
    "        # student train processing\n",
    "        train_step.run(feed_dict={x_ :batch[0], y_: batch[1], T : params_t, y_soft_target:soft_target.eval() })\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            hd_loss, sf_loss, loss_num, train_accuracy = sess_student.run([hard_loss, soft_loss ,Loss, accuracy], \n",
    "                                                        feed_dict={x_:batch[0],  y_:batch[1],\n",
    "                                                                   T:1.0, y_soft_target:soft_target.eval()  }) \n",
    "            print('step %d, training accuracy %g , loss = %g , hard_loss = %g, soft_loss = %g' % \n",
    "                       (i, train_accuracy, loss_num, hd_loss, sf_loss ))\n",
    "        if i % 1000 == 0:\n",
    "            print('test accuracy %g' % sess_student.run(accuracy,feed_dict={\n",
    "                                    x_: mnist.test.images, y_: mnist.test.labels, T: 1.0}))\n",
    "            \n",
    "    print('Finally - test accuracy %g' % sess_student.run(accuracy,feed_dict={\n",
    "                                    x_: mnist.test.images, y_: mnist.test.labels, T: 1.0}))\n",
    "           \n",
    "    \n",
    "sess_teacher.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
